{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf2fdf1",
   "metadata": {},
   "source": [
    "# Notebook principal\n",
    "\n",
    "Sistema de predicción espacio temporal de altos riesgos delictivos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444259e5",
   "metadata": {},
   "source": [
    "# Limpieza datos ECU911\n",
    "\n",
    "limpieza_de_datos_ecu_911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376cf105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIGURACIÓN\n",
    "# -----------------------------\n",
    "RUTA_PADRE = \"../data\"\n",
    "RUTA_ECU911_RAW = os.path.join(RUTA_PADRE, \"raw\", \"ecu911\", \"dataset\")\n",
    "PATRON_CSV = os.path.join(RUTA_ECU911_RAW, \"*.csv\")\n",
    "\n",
    "SALIDA_LIMPIA = os.path.join(RUTA_PADRE, \"interim\", \"ecu911\", \"ecu911_limpio.csv\")\n",
    "os.makedirs(os.path.dirname(SALIDA_LIMPIA), exist_ok=True)\n",
    "\n",
    "archivos_csv = glob.glob(PATRON_CSV)\n",
    "print(f\"Archivos CSV encontrados: {len(archivos_csv)}\")\n",
    "print(\"Ejemplo:\", archivos_csv[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16646750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_columnas (df: pd.DataFrame):\n",
    "  df = df.copy()\n",
    "  df.columns = (\n",
    "    df.columns.astype(str)\n",
    "    .str.replace(\"ï»¿\", \"\", regex=False)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "  )\n",
    "  return df\n",
    "\n",
    "def limpiar_texto_basico(serie: pd.Series) -> pd.Series:\n",
    "  s = serie.astype(str).str.strip()\n",
    "  s = s.replace({\"\": pd.NA, \"nan\": pd.NA, \"none\": pd.NA, \"null\": pd.NA})\n",
    "  return s\n",
    "\n",
    "def estandarizar_cod_parroquia(serie: pd.Series) -> pd.Series:\n",
    "  s = limpiar_texto_basico(serie)\n",
    "  s = s.str.replace(\".0\", \"\", regex=False)\n",
    "  s = s.str.zfill(6)\n",
    "  return s\n",
    "\n",
    "def parsear_fecha(serie: pd.Series) -> pd.Series:\n",
    "    # Convierte a datetime de forma robusta\n",
    "    return pd.to_datetime(serie, errors=\"coerce\", dayfirst=True)\n",
    "\n",
    "\n",
    "def limpiar_lat_lon(df: pd.DataFrame, col_lat=\"lat\", col_lon=\"lon\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Limpia lat/lon si existen: convierte coma->punto, numérico, elimina nulos,\n",
    "    elimina ceros y rangos inválidos.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    if col_lat not in df.columns or col_lon not in df.columns:\n",
    "        print(f\"Aviso: no existen columnas {col_lat}/{col_lon}. Se omite limpieza lat/lon.\")\n",
    "        return df\n",
    "\n",
    "    for col in [col_lat, col_lon]:\n",
    "        df[col] = (\n",
    "            df[col]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.replace(\",\", \".\", regex=False)\n",
    "        )\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "    antes = len(df)\n",
    "    df = df.dropna(subset=[col_lat, col_lon])\n",
    "\n",
    "    # Quitar ceros (como en tu ejemplo)\n",
    "    df = df[(df[col_lat] != 0) & (df[col_lon] != 0)]\n",
    "\n",
    "    # Rangos válidos geográficos\n",
    "    df = df[(df[col_lat].between(-90, 90)) & (df[col_lon].between(-180, 180))]\n",
    "\n",
    "    print(f\"Limpieza lat/lon: eliminados {antes - len(df)} registros inválidos.\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def leer_csv_ecu911(path_csv: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Lee CSV ECU911 con separador ';', normaliza columnas y aplica filtro\n",
    "    (si aplica) sin crear features.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(\n",
    "        path_csv,\n",
    "        sep=\";\",\n",
    "        encoding=\"UTF-8\",\n",
    "        on_bad_lines=\"skip\",\n",
    "        low_memory=False\n",
    "    )\n",
    "    df = normalizar_columnas(df)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68807d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "errores = []\n",
    "\n",
    "print(\"Cargando archivos ECU911... esto puede tardar unos segundos...\")\n",
    "\n",
    "for archivo in archivos_csv:\n",
    "    try:\n",
    "        df_tmp = leer_csv_ecu911(archivo)\n",
    "        # df_tmp[\"__source_file\"] = os.path.basename(archivo)  # trazabilidad (opcional)\n",
    "        dfs.append(df_tmp)\n",
    "    except Exception as e:\n",
    "        errores.append((archivo, str(e)))\n",
    "\n",
    "if errores:\n",
    "    print(f\"Archivos con error: {len(errores)}\")\n",
    "    print(\"Ejemplo:\", errores[0])\n",
    "\n",
    "df_911 = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "print(f\"Registros totales unificados: {len(df_911)}\")\n",
    "print(f\"Columnas: {list(df_911.columns)}\")\n",
    "df_911.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d8bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1) FILTRO OPCIONAL (si forma parte de limpieza por alcance)\n",
    "# -----------------------------\n",
    "if \"servicio\" in df_911.columns:\n",
    "    antes = len(df_911)\n",
    "    df_911[\"servicio\"] = limpiar_texto_basico(df_911[\"servicio\"])\n",
    "    df_911 = df_911[df_911[\"servicio\"] == \"Seguridad Ciudadana\"]\n",
    "    print(f\"Filtro servicio=Seguridad Ciudadana: eliminados {antes - len(df_911)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) LIMPIEZA DE FECHA\n",
    "# -----------------------------\n",
    "if \"fecha\" in df_911.columns:\n",
    "    # Parsear fecha original\n",
    "    df_911[\"fecha_dt\"] = parsear_fecha(df_911[\"fecha\"])\n",
    "    \n",
    "    antes = len(df_911)\n",
    "    df_911 = df_911.dropna(subset=[\"fecha_dt\"])\n",
    "    print(f\"Fechas inválidas eliminadas: {antes - len(df_911)}\")\n",
    "    \n",
    "    # Eliminar columna original y renombrar\n",
    "    df_911 = df_911.drop(columns=[\"fecha\"])\n",
    "    df_911 = df_911.rename(columns={\"fecha_dt\": \"fecha\"})\n",
    "    \n",
    "    # Asegurar tipo datetime\n",
    "    df_911[\"fecha\"] = pd.to_datetime(df_911[\"fecha\"])\n",
    "else:\n",
    "    print(\"Aviso: no existe columna 'fecha'.\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3) LIMPIEZA DE COD_PARROQUIA (solo formato, sin cruce)\n",
    "# -----------------------------\n",
    "# Nota: tras normalizar columnas, normalmente quedará 'cod_parroquia' aunque viniera como 'Cod_Parroquia'\n",
    "if \"cod_parroquia\" in df_911.columns:\n",
    "    df_911[\"cod_parroquia\"] = estandarizar_cod_parroquia(df_911[\"cod_parroquia\"])\n",
    "    antes = len(df_911)\n",
    "    df_911 = df_911.dropna(subset=[\"cod_parroquia\"])\n",
    "    print(f\"Códigos de parroquia vacíos eliminados: {antes - len(df_911)}\")\n",
    "else:\n",
    "    # Esto es útil para diagnosticar cómo se llama realmente\n",
    "    posibles = [c for c in df_911.columns if \"parroq\" in c or \"parro\" in c]\n",
    "    print(\"Aviso: no existe 'cod_parroquia'. Candidatas:\", posibles)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca47fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intenta limpiar lat/lon con nombres típicos: lat/lon\n",
    "df_911 = limpiar_lat_lon(df_911, col_lat=\"lat\", col_lon=\"lon\")\n",
    "\n",
    "# Si tu dataset usa \"latitud\"/\"longitud\" en vez de lat/lon, usa esto:\n",
    "# df_911 = limpiar_lat_lon(df_911, col_lat=\"latitud\", col_lon=\"longitud\")\n",
    "\n",
    "print(f\"Registros después de limpieza lat/lon (si aplicó): {len(df_911)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb00883d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_911.to_csv(SALIDA_LIMPIA, index=False, encoding=\"utf-8\")\n",
    "    print(\"Dataset limpio guardado correctamente.\")\n",
    "    print(\"Salida:\", SALIDA_LIMPIA)\n",
    "    print(\"Registros finales:\", len(df_911))\n",
    "except Exception as e:\n",
    "    print(\"Error guardando el CSV:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c3aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "errores = []\n",
    "\n",
    "print(\"Cargando archivos ECU911... esto puede tardar unos segundos...\")\n",
    "\n",
    "for archivo in archivos_csv:\n",
    "    try:\n",
    "        df_tmp = leer_csv_ecu911(archivo)\n",
    "        # df_tmp[\"__source_file\"] = os.path.basename(archivo)  # trazabilidad (opcional)\n",
    "        dfs.append(df_tmp)\n",
    "    except Exception as e:\n",
    "        errores.append((archivo, str(e)))\n",
    "\n",
    "if errores:\n",
    "    print(f\"Archivos con error: {len(errores)}\")\n",
    "    print(\"Ejemplo:\", errores[0])\n",
    "\n",
    "df_911 = pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "print(f\"Registros totales unificados: {len(df_911)}\")\n",
    "print(f\"Columnas: {list(df_911.columns)}\")\n",
    "df_911.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b2ed96",
   "metadata": {},
   "source": [
    "# ECU911 preprocesamiento con coords\n",
    "\n",
    "preprocesamiento_datos_ecu911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4103c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "RUTA_PADRE = \"../data\"\n",
    "\n",
    "RUTA_LIMPIO = os.path.join(RUTA_PADRE, \"interim\", \"ecu911\", \"ecu911_limpio.csv\")\n",
    "RUTA_CATALOGO = os.path.join(RUTA_PADRE, \"raw\", \"catalogo_parroquias_ecuador.csv\")\n",
    "\n",
    "SALIDA_PREPRO = os.path.join(RUTA_PADRE, \"processed\", \"ecu911\", \"ecu911_con_coords.csv\")\n",
    "os.makedirs(os.path.dirname(SALIDA_PREPRO), exist_ok=True)\n",
    "\n",
    "print(\"Ruta limpio:\", RUTA_LIMPIO)\n",
    "print(\"Ruta catálogo:\", RUTA_CATALOGO)\n",
    "print(\"Salida:\", SALIDA_PREPRO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizar_columnas(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.astype(str)\n",
    "        .str.replace(\"ï»¿\", \"\", regex=False)\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def estandarizar_cod_parroquia(serie: pd.Series) -> pd.Series:\n",
    "    s = serie.astype(str).str.strip()\n",
    "    s = s.str.replace(\".0\", \"\", regex=False)\n",
    "    s = s.str.zfill(6)\n",
    "    s = s.replace({\"nan\": pd.NA, \"none\": pd.NA, \"\": pd.NA})\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e38536",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Cargando dataset limpio...\")\n",
    "    df_911 = pd.read_csv(RUTA_LIMPIO, low_memory=False)\n",
    "    df_911 = normalizar_columnas(df_911)\n",
    "    print(\"Registros:\", len(df_911))\n",
    "    print(\"Columnas:\", list(df_911.columns))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error cargando dataset limpio: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb42799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Cargando catálogo de parroquias...\")\n",
    "    catalogo = pd.read_csv(RUTA_CATALOGO, dtype={\"cod_parroquia\": str}, low_memory=False)\n",
    "    catalogo = normalizar_columnas(catalogo)\n",
    "    print(\"Filas catálogo:\", len(catalogo))\n",
    "    print(\"Columnas catálogo:\", list(catalogo.columns))\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error cargando catálogo: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2166452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validar que exista cod_parroquia en ambos\n",
    "if \"cod_parroquia\" not in df_911.columns:\n",
    "    raise ValueError(\"El dataset limpio no tiene 'cod_parroquia'. Revisa nombres de columnas.\")\n",
    "\n",
    "if \"cod_parroquia\" not in catalogo.columns:\n",
    "    raise ValueError(\"El catálogo no tiene 'cod_parroquia'. Revisa nombres de columnas.\")\n",
    "\n",
    "# Estandarizar\n",
    "df_911[\"cod_parroquia\"] = estandarizar_cod_parroquia(df_911[\"cod_parroquia\"])\n",
    "catalogo[\"cod_parroquia\"] = estandarizar_cod_parroquia(catalogo[\"cod_parroquia\"])\n",
    "\n",
    "# Validar lat/lon\n",
    "# Si en tu catálogo se llaman distinto, cámbialos aquí.\n",
    "lat_col = \"lat\"\n",
    "lon_col = \"lon\"\n",
    "\n",
    "if lat_col not in catalogo.columns or lon_col not in catalogo.columns:\n",
    "    raise ValueError(f\"El catálogo no tiene columnas '{lat_col}'/'{lon_col}'. Columnas disponibles: {list(catalogo.columns)}\")\n",
    "\n",
    "# Mantener solo lo necesario del catálogo para evitar columnas repetidas\n",
    "catalogo_coords = catalogo[[\"cod_parroquia\", lat_col, lon_col]].drop_duplicates(subset=[\"cod_parroquia\"])\n",
    "print(\"Catálogo coords (únicos por cod_parroquia):\", len(catalogo_coords))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "antes = len(df_911)\n",
    "\n",
    "df_merge = df_911.merge(\n",
    "    catalogo_coords,\n",
    "    on=\"cod_parroquia\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Registros tras merge:\", len(df_merge), \"(debería ser igual a antes:\", antes, \")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bc9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = len(df_merge)\n",
    "sin_latlon = df_merge[lat_col].isna().sum()\n",
    "\n",
    "print(\"Total registros:\", total)\n",
    "print(\"Sin coordenadas:\", sin_latlon)\n",
    "print(\"Con coordenadas:\", total - sin_latlon)\n",
    "print(\"Match rate:\", f\"{(total - sin_latlon)/total*100:.2f}%\")\n",
    "\n",
    "# Top parroquias sin match (para depuración)\n",
    "top_sin_match = (\n",
    "    df_merge[df_merge[lat_col].isna()]\n",
    "    .groupby(\"cod_parroquia\")\n",
    "    .size()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(15)\n",
    ")\n",
    "\n",
    "print(\"\\nTop cod_parroquia sin match (conteo de llamadas):\")\n",
    "print(top_sin_match)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b7d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df_merge.sort_values(by=\"fecha\", ascending=True)\n",
    "df_merge = df_merge.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da96a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcional: quedarte solo con filas con coordenadas\n",
    "df_final = df_merge.dropna(subset=[lat_col, lon_col]).copy()\n",
    "\n",
    "df_final.to_csv(SALIDA_PREPRO, index=False, encoding=\"utf-8\")\n",
    "print(\"Guardado:\", SALIDA_PREPRO)\n",
    "print(\"Registros finales con coords:\", len(df_final))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d10fe86",
   "metadata": {},
   "source": [
    "# ECU911 subtipos frecuentes\n",
    "\n",
    "exploracion_ecu_911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4011a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "RUTA_PADRE = \"../data\"\n",
    "\n",
    "RUTA_LIMPIO = os.path.join(RUTA_PADRE, \"processed\", \"ecu911\", \"ecu911_con_coords.csv\")\n",
    "\n",
    "\n",
    "SALIDA_CONSULTA = os.path.join(RUTA_PADRE, \"processed\", \"ecu911\", \"ecu911_subtipos_frecuentes.csv\")\n",
    "os.makedirs(os.path.dirname(SALIDA_CONSULTA), exist_ok=True)\n",
    "\n",
    "print(\"Ruta limpio:\", RUTA_LIMPIO)\n",
    "\n",
    "print(\"Salida:\", SALIDA_CONSULTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f25c8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecu911 = pd.read_csv(RUTA_LIMPIO)\n",
    "subtipos_frecuentes = df_ecu911['subtipo'].value_counts().reset_index()\n",
    "subtipos_frecuentes.columns = ['subtipo', 'frecuencia']\n",
    "\n",
    "subtipos_frecuentes.to_csv(SALIDA_CONSULTA, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985ae7bb",
   "metadata": {},
   "source": [
    "# Preprocesamiento aprehendidos/detenidos\n",
    "\n",
    "preprocesamiento_detenidosaprehendidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d71ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#carga de datos\n",
    "nombre_archivo = '../data/raw/aprehendidos_detenidos/dataset/mdi_detenidosaprehendidos_pm_2025_enero_octubre.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018930e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Cargando archivo Excel... esto puede tardar unos segundos...\")\n",
    "    df_apre = pd.read_excel(\n",
    "        nombre_archivo,\n",
    "        sheet_name=1,\n",
    "        dtype={'codigo_parroquia': str},  # proteger columna\n",
    "        engine='openpyxl'\n",
    "    )\n",
    "    print(f\"¡Cargado! Registros totales: {len(df_apre)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error cargando el Excel:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202387e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalización de datos de latitud y longitud\n",
    "for col in [\"latitud\", \"longitud\"]:\n",
    "    df_apre[col] = (\n",
    "        df_apre[col]\n",
    "        .astype(str)\n",
    "        .str.replace(\",\", \".\", regex=False)\n",
    "    )\n",
    "    df_apre[col] = pd.to_numeric(df_apre[col], errors=\"coerce\")\n",
    "\n",
    "# quitar filas sin coordenadas válidas\n",
    "df_apre = df_apre.dropna(subset=[\"latitud\", \"longitud\"])\n",
    "df_apre = df_apre[(df_apre[\"latitud\"] != 0) & (df_apre[\"longitud\"] != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f49e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nromalizar fecha y hora de detención/aprehensión\n",
    "try:\n",
    "    df_apre[\"fecha\"] = pd.to_datetime(df_apre[\"fecha_detencion_aprehension\"], errors=\"coerce\")\n",
    "\n",
    "    # hora como string limpio\n",
    "    df_apre[\"hora_limpia\"] = (\n",
    "        df_apre[\"hora_detencion_aprehension\"]\n",
    "        .astype(str)\n",
    "        .str.replace(\" \", \"\")\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # Combinar fecha + hora\n",
    "    df_apre[\"fecha_completa\"] = pd.to_datetime(\n",
    "        df_apre[\"fecha\"].astype(str) + \" \" + df_apre[\"hora_limpia\"],\n",
    "        errors=\"coerce\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Advertencia procesando fecha/hora:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53477949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#estandarizar el codd parroquia\n",
    "if \"codigo_parroquia\" in df_apre.columns:\n",
    "    df_apre[\"codigo_parroquia\"] = (\n",
    "        df_apre[\"codigo_parroquia\"]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "        .str.zfill(6)\n",
    "    )\n",
    "\n",
    "\n",
    "#columnas de interes\n",
    "cols_interes = [\n",
    "    \"fecha_completa\", \"fecha\", \"latitud\", \"longitud\",\n",
    "    \"codigo_parroquia\", \"nombre_parroquia\",\n",
    "    \"presunta_infraccion\", \"tipo\", \"arma\", \"movilizacion\"\n",
    "]\n",
    "\n",
    "df_apre_clean = df_apre[cols_interes].copy()\n",
    "\n",
    "# features temporales\n",
    "df_apre_clean[\"franja_horaria\"] = df_apre_clean[\"fecha_completa\"].dt.hour\n",
    "df_apre_clean[\"dia\"] = df_apre_clean[\"fecha_completa\"].dt.day\n",
    "df_apre_clean[\"mes\"] = df_apre_clean[\"fecha_completa\"].dt.month\n",
    "df_apre_clean[\"dia_semana\"] = df_apre_clean[\"fecha_completa\"].dt.dayofweek  # 0=Lunes\n",
    "\n",
    "#grid espacial\n",
    "df_apre_clean[\"lat_grid\"] = df_apre_clean[\"latitud\"].round(3)\n",
    "df_apre_clean[\"lon_grid\"] = df_apre_clean[\"longitud\"].round(3)\n",
    "\n",
    "\n",
    "#conteo de delios por dia y zona\n",
    "grouped = (\n",
    "    df_apre_clean\n",
    "    .groupby([\"lat_grid\", \"lon_grid\", \"fecha\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"conteo_delitos\")\n",
    ")\n",
    "\n",
    "df_apre_clean = df_apre_clean.merge(\n",
    "    grouped,\n",
    "    on=[\"lat_grid\", \"lon_grid\", \"fecha\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "#guardar dataset limpio\n",
    "print(\"\\n=== Dataset final limpio ===\")\n",
    "print(df_apre_clean.head())\n",
    "print(f\"Total registros finales: {len(df_apre_clean)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b92087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"--- Primeras Filas---\")\n",
    "print(df_apre_clean.head())\n",
    "print(\"\\n--- Tipos de datos ---\")\n",
    "print(df_apre_clean.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Validación Geográfica Rápida\n",
    "print(\"--- Rango de Coordenadas ---\")\n",
    "print(f\"Latitud: {df_apre_clean['latitud'].min()} a {df_apre_clean['latitud'].max()}\")\n",
    "print(f\"Longitud: {df_apre_clean['longitud'].min()} a {df_apre_clean['longitud'].max()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6afe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 3. Delitos más comunes\n",
    "print(\"\\n--- Top 10 Infracciones ---\")\n",
    "print(df_apre_clean['presunta_infraccion'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f3c851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASIFICACIÓN DE DELITOS GRAVES\n",
    "# ============================================================\n",
    "\n",
    "delitos_interes = [\n",
    "    'DELITOS CONTRA EL DERECHO A LA PROPIEDAD',\n",
    "    'DELITOS POR LA PRODUCCIÓN O TRÁFICO ILÍCITO DE SUSTANCIAS CATALOGADAS SUJETAS A FISCALIZACIÓN',\n",
    "    'DELITOS CONTRA LA SEGURIDAD PÚBLICA',\n",
    "    'DELITOS CONTRA LA EFICIENCIA DE LA ADMINISTRACIÓN PÚBLICA',\n",
    "    'DELITOS DE VIOLENCIA CONTRA LA MUJER O MIEMBROS DEL NÚCLEO FAMILIAR'\n",
    "]\n",
    "\n",
    "# 1. Etiqueta de delito grave (0/1)\n",
    "df_apre_clean[\"es_delito_grave\"] = df_apre_clean[\"presunta_infraccion\"].isin(delitos_interes).astype(int)\n",
    "\n",
    "# 2. Conteo solo de delitos graves (target alternativo)\n",
    "df_graves = (\n",
    "    df_apre_clean[df_apre_clean[\"es_delito_grave\"] == 1]\n",
    "    .groupby([\"lat_grid\", \"lon_grid\", \"fecha\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"conteo_delitos_graves\")\n",
    ")\n",
    "\n",
    "# Asegurar que no exista antes de unir\n",
    "if \"conteo_delitos_graves\" in df_apre_clean.columns:\n",
    "    df_apre_clean.drop(columns=[\"conteo_delitos_graves\"], inplace=True)\n",
    "\n",
    "# Merge limpio\n",
    "df_apre_clean = df_apre_clean.merge(\n",
    "    df_graves,\n",
    "    on=[\"lat_grid\", \"lon_grid\", \"fecha\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Rellenar NaN con 0 (ningún delito grave en esa celda y día)\n",
    "df_apre_clean[\"conteo_delitos_graves\"] = df_apre_clean[\"conteo_delitos_graves\"].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c98fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# LIMPIEZA DE POSIBLES DUPLICADOS (col_x, col_y)\n",
    "# ============================================================\n",
    "\n",
    "cols_a_borrar = [c for c in df_apre_clean.columns if c.endswith(\"_x\") or c.endswith(\"_y\")]\n",
    "\n",
    "if len(cols_a_borrar) > 0:\n",
    "    print(f\"\\nEliminando columnas duplicadas generadas por merge: {cols_a_borrar}\")\n",
    "    df_apre_clean.drop(columns=cols_a_borrar, inplace=True)\n",
    "\n",
    "\n",
    "print(\"\\n--- Distribución de delitos graves (solo para análisis) ---\")\n",
    "print(df_apre_clean[\"es_delito_grave\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf56fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# GUARDADO (solo CSV)\n",
    "# ============================================================\n",
    "\n",
    "df_apre_clean.to_csv(\"../data/interim/aprehendidos_Detenidos/aprehendidos_limpio_final.csv\", index=False)\n",
    "\n",
    "print(\"\\n¡Archivo 'aprehendidos_limpio_final.csv' guardado con éxito!\")\n",
    "print(f\"Registros finales: {len(df_apre_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77166539",
   "metadata": {},
   "source": [
    "# Preprocesamiento conteo dias conteo\n",
    "\n",
    "preprocesamiento_ecu911\n",
    "\n",
    "## VOLVER A CORREGIR LO DE VOLVER A CONCATENAR LOS ARCHIVOS DE ECU911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "ruta_carpeta_911 = \"../data/raw/ecu911/dataset/\"\n",
    "ruta_catalogo = \"../data/processed/\"\n",
    "ruta_dest = \"../data/processed/ecu911\"\n",
    "\n",
    "archivos_csv = glob.glob(os.path.join(ruta_carpeta_911, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2880b58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lista_dfs = []\n",
    "\n",
    "for archivo in archivos_csv:\n",
    "    df_temp = pd.read_csv(\n",
    "        archivo,\n",
    "        sep=\";\",\n",
    "        encoding=\"UTF-8\",\n",
    "        dtype={\"Cod_Parroquia\": str},\n",
    "        on_bad_lines=\"skip\"\n",
    "    )\n",
    "\n",
    "    df_temp.columns = (\n",
    "        df_temp.columns.astype(str)\n",
    "        .str.replace(\"ï»¿\", \"\")\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "    )\n",
    "\n",
    "    if \"servicio\" in df_temp.columns:\n",
    "        df_temp = df_temp[df_temp[\"servicio\"] == \"Seguridad Ciudadana\"]\n",
    "\n",
    "    lista_dfs.append(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff015462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_911 = pd.concat(lista_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dc2baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fecha\n",
    "df_911[\"fecha_dt\"] = pd.to_datetime(df_911[\"fecha\"], errors=\"coerce\", dayfirst=True)\n",
    "df_911 = df_911.dropna(subset=[\"fecha_dt\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2641cfb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar cod_parroquia\n",
    "df_911[\"cod_parroquia\"] = (\n",
    "    df_911[\"cod_parroquia\"]\n",
    "    .astype(str)\n",
    "    .str.replace(\".0\", \"\", regex=False)\n",
    "    .str.zfill(6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ea361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (CLAVE) agrupar por parroquia y día (si quieres día, normaliza a date)\n",
    "df_911[\"fecha_dia\"] = df_911[\"fecha_dt\"].dt.date\n",
    "\n",
    "df_group = (\n",
    "    df_911.groupby([\"cod_parroquia\", \"fecha_dia\"])\n",
    "    .size()\n",
    "    .reset_index(name=\"conteo_llamadas_riesgo\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c854d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Catálogo: asegurar 1 fila por cod_parroquia antes de merge\n",
    "catalogo = pd.read_csv(\n",
    "    os.path.join(ruta_catalogo, \"catalogo_parroquias_ecuador.csv\"),\n",
    "    dtype={\"cod_parroquia\": str}\n",
    ")\n",
    "\n",
    "catalogo[\"cod_parroquia\"] = catalogo[\"cod_parroquia\"].astype(str).str.zfill(6)\n",
    "\n",
    "# si hay repetidos, quédate con uno (o define una regla mejor si aplica)\n",
    "catalogo = catalogo.drop_duplicates(subset=[\"cod_parroquia\"], keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e177d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge para lat/lon (ahora NO explota)\n",
    "df_final = df_group.merge(catalogo[[\"cod_parroquia\", \"lat\", \"lon\"]], on=\"cod_parroquia\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662c594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar sin coords si quieres\n",
    "df_final = df_final.dropna(subset=[\"lat\", \"lon\"])\n",
    "\n",
    "#grid espacial\n",
    "df_final[\"lat_grid\"] = df_final[\"lat\"].round(3)\n",
    "df_final[\"lon_grid\"] = df_final[\"lon\"].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2827253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features temporales sobre fecha_dia\n",
    "df_final[\"fecha_dia\"] = pd.to_datetime(df_final[\"fecha_dia\"])\n",
    "df_final[\"mes\"] = df_final[\"fecha_dia\"].dt.month\n",
    "df_final[\"dia\"] = df_final[\"fecha_dia\"].dt.day\n",
    "df_final[\"dia_semana\"] = df_final[\"fecha_dia\"].dt.dayofweek\n",
    "\n",
    "df_final.to_csv(os.path.join(ruta_dest, \"ecu911_parroquia_dia_conteo.csv\"), index=False)\n",
    "\n",
    "print(\"ECU911 agregado por parroquia y día correctamente\")\n",
    "print(\"Registros finales:\", len(df_final))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
